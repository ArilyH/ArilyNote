## 研究问题
研究对回答引入语法错误且不改变语义的情况下，是否可以影响JudgeLLM的评分。
研究对回答引入同义词替换且不改变语义的情况下，是否可以影响JudgeLLM的评分。
研究不同的JudgeLLM是否对一系列同义词存在不同的偏好。
研究不同的JudgeLLM是否有（写作风格偏好），可以尝试用LLM生成。
是否可以从写作风格偏好的角度攻击LLM（如在本地部署的蒸馏模型上进行训练，得出同义词偏好列表，并尝试以此修改回答，攻击在线LLM）。
如何防范此类攻击。

### 一、研究问题分析

#### **1. 语法错误对JudgeLLM评分的影响**

- **研究意义**：语法错误是否会影响JudgeLLM的评分，直接关系到模型的鲁棒性和可靠性。例如，模型可能因语法错误而低估内容的逻辑性或流畅性，也可能因容错能力过强而忽略错误。
    
- **技术背景**：研究表明，语法错误会干扰文本的可读性和理解，甚至可能导致模型对语义的误判5。但现有研究多关注人类对语法错误的认知，对LLM的敏感性分析较少。
    
    
    - **实验设计**：需区分错误类型（如主谓一致、时态错误）和扰动强度（轻度、重度），并对比不同JudgeLLM（如GPT-4、Claude）的评分差异。
        
    - **评估指标**：引入动态权重评估框架（如DSGram），结合语义一致性、流畅性等多维度指标，避免仅依赖单一评分。
        

#### **2. 同义词替换对JudgeLLM评分的影响**

- **研究意义**：同义词替换可能暴露模型对特定词汇或表达风格的偏好（如“优秀” vs. “卓越”），进而影响公平性。
    
- **技术背景**：同义词替换在论文降重中效果有限，因查重算法已能检测深层语义相似性，但LLM可能对词汇风格更敏感。例如，某些模型可能偏好学术化词汇而非口语表达。
    
    
    - **实验变量**：需控制语义不变性（通过SBert等模型验证），并设计不同替换策略（如学术词汇 vs. 日常词汇）。
        
    - **数据构建**：使用人工标注或对抗生成工具（如小发猫伪原创）生成同义词扰动样本。
        

#### **3. JudgeLLM的同义词偏好差异**

- **研究意义**：不同模型对同义词的偏好可能反映其训练数据分布或对齐目标差异，例如教育领域模型更偏好正式表达。
    
- **技术背景**：现有研究指出，模型对结构化输出（如JSON）的偏好可能影响其生成效率，但对词汇风格的偏好尚缺乏系统性分析。
    
    
    - **偏好量化**：通过蒸馏模型提取不同JudgeLLM的注意力权重或生成概率分布，构建“偏好词表”7。
        
    - **跨模型对比**：对比开源模型（如LLaMA）与闭源模型（如GPT-4）的偏好差异。
        

#### **4. 基于写作风格偏好的攻击与防御**

- **攻击可行性**：若模型存在显著的同义词偏好，攻击者可通过替换偏好词汇操纵评分（如将负面评价中的“糟糕”替换为“欠佳”）。
    
- **防御挑战**：传统防御方法（如困惑度检测）易被绕过，需结合语义和逻辑一致性检测。
    
    
    - **攻击实验**：在本地蒸馏模型上训练对抗样本，生成“偏好优化”文本，测试在线模型的评分偏移6。
        
    - **防御策略**：
        
        - **多维度检测**：整合语义相似度（SBert）、逻辑一致性（如DSGram的动态权重。
            
        - **对抗性训练**：在训练数据中引入风格扰动样本，增强模型对偏好的鲁棒性。


## 语法错误扰动
- **拼写错误**
    - 示例：`exmaple` 替代 `example`。
- **时态错误**
    - 示例：`He go to school yesterday.` 替代 `He went to school yesterday.`。
- **主谓不一致**
    - 示例：`The data are accurate.` 替代 `The data is accurate.`。
- **词序错误**
    - 示例：`She quickly runs.` 替代 `Quickly she runs.`。
- **冗余或缺失词语**
    - 示例：`I not understand.` 替代 `I do not understand.`。

```
- **Spelling Errors**
    
    - Example: `exmaple` instead of `example`.
- **Tense Errors**
    
    - Example: `He go to school yesterday.` instead of `He went to school yesterday.`
- **Subject-Verb Agreement Errors**
    
    - Example: `The data are accurate.` instead of `The data is accurate.`
- **Word Order Errors**
    
    - Example: `She quickly runs.` instead of `Quickly she runs.`
- **Redundant or Missing Words**
    
    - Example: `I not understand.` instead of `I do not understand.`
```

## 扰动等级
- **轻度噪声**：1-2 个语法错误，分布于整个回答。
- **中度噪声**：3-5 个语法错误，分布于整个回答。
- **重度噪声**：每句话至少一个语法错误。
- **可控错乱**：将语法错误集中在特定位置，如回答开头或结尾，测试 LLM Judge 的敏感度。


## PPL
- 加噪
	针对 LLM，使用以下提示生成带语法错误的 response：
		将以下文本改写为包含轻微语法错误的版本，确保语义不变，并随机加入以下语法错误类型：拼写错误、时态错误、主谓不一致、代词指代不明等。错误数量不超过 2 个。
		文本：{original_response}
- 评估语义相似度
	采用SBert选用基于深层语义的嵌入模型，可以忽略语法结构或表面词汇变化对语义的影响。
- Bench
	使用加噪后的Bench评估LLM Judge

## Metrics
- Robust Rate: 扰动前后的Judge结果差异。
- Consistent Rate: 特定条件下的一致性差异。

## Exp
### 鲁棒性评价
评价不同扰动强度影响下现有Judge抗扰动的性能。

### 偏好评价
评价现有Judge在哪些错误类型下表现更好。

### Ablation
- 扰动模型与Judge模型同源状况下的影响。


