---
date: 2024-12-16
tags:
  - LLM
  - Judge
  - Bench
---
## 1. Links

GitHub: [GitHub - ScalerLab/JudgeBench --- GitHub - ScalerLab/JudgeBench](https://github.com/ScalerLab/JudgeBench)
arXiv: [2410.12784](https://arxiv.org/pdf/2410.12784)

## 2. Intro

**现状**：
由于收集人类评估是高成本且耗时的，且人类评估可能引入潜在的偏见。LLM Judge成为人类评估的可替代方案。并越来越多地应用于（[[RLHF]]）评估、改善现有LLM的表现。

现有的BenchMark主要关注LLM与人类偏好的契合度，但这只是空中楼阁（见[[Judge Bench#^Key1|Key1]]），更有可能是因为它们不能找到对正确性要求更高的Bench。
人类评估者在问题变得更为复杂时往往是不可信的。
更预见性地说，将 AI 模型扩展到超人类水平需要相应地发展 AI Judge，而非仅仅依赖于人类Judge，以便准确评估这些LLM复杂的回答。

需要一些更可靠的Bench来根据Judge的推理能力以客观、无偏见的方式评估LLM Judge。
（换句话说就是你需要推理上来说更难的Bench就对了）

**Key**：
**1 LLM的回答什么时候是正确的？**
（1）回答必须忠实遵循人类指令。
（2）它应提供事实和逻辑上正确答案。
（3）其风格应与人类偏好一致。
(1) > (2) > (3)
^Key1

**2 Judge在什么问题上是更Struggle的？**
如果一个模型难以持续生成针对具有挑战性的问题的正确、连贯的回应，它将难以区分这些回应。
（你不会你怎么评价？）

**目标**：
将现有的难数据集转化成可用的LLM Judge测试数据集，以改善LLM Judge难数据集缺失的现状。

## 3. Related Work
**LLM Judge 分类：**
	PromptJudge 依赖于精心制作的提示来指导LLMs充当Judge，利用底层模型的内在能力
	FineTunedJudge 在特定的偏好数据集上接受训练 泛化性不够
	MultiAgentJudge 使用多个LLMs来生成判断 计算成本高

**Reward Model:**
	在大语言模型（LLM）的训练过程中，**Reward Model(奖励模型)** 是一个从基础语言模型（如 GPT）监督学习微调而来的判别模型是一种核心工具，用于将人类偏好信号转化为数值奖励，指导模型生成更符合人类期望的输出。
	[[RewardModel.png|回馈模型]] [[LLMJudge.png|LLMJudge]]

## 4. Methods
**Key**：
利用一个具有真实标签的现有难LLM数据集，并开发一个流水线将其转换为一系列回答对。只要数据集包含真实标签和验证正确性的算法，则可以构造回答对：其中一个响应通过了验证检查，而另一个没有。

**Goal**
构建数对Judge难以区分的回答对，以重点客观评判模型判定问题回答的指令完成度及回答正确性。

**Proposal**
-[x] 使用多个LLMs来生成候选回答，并选择一个正确和一个错误的回答来形成一对。
```
Why not？
1. 不同模型的能力各异，错误的回答可能很容易被识别，从而降低数据集的难度。
2. 每个模型的回答都有其独特的风格，如此构造数据集可能会导致Judge错误地优先学习到风格差异，而不是评估答案的事实正确性。
3. Judge倾向于青睐由同一模型生成的回答。这可能引入自反偏差。引入多个模型进行数据生成将难以定位自反偏差。
```
-[√] 使用单个强LLM来对某一问题生成多个候选回答，并选择一个正确和一个错误的回答来形成一对。
```
Why？
1. 也确保了回答风格的连贯性，减少了风格差异的影响。
2. 减轻了自我提升偏见对判断的影响。

Why not？
1. 可能对特定模型具有不成比例的挑战性。
2. 不同的模型可能不会在相同的问题上遇到困难。
尽管如此，偏见仅限于用于生成回答的模型，而尽量为所有其他模型创造了一个公平竞争的环境。
但问题仍然可能存在于Judge中。基础模型有能力生成正确的回答，必定由于基础模型对于该类问题有着较强的推理能力。与基础模型的强处相似的模型更可能取得优势。

```
^Bias
```
PPL
1. 给定来自现有数据集的问题，从强大的模型（如 GPT-4o）中采样 k 个答案。
2. 评估每个答案的正确性。
3. 留下所有至少有一个正确答案和一个错误答案的问题，从而构建具有客观真实标签的答案对。

Mark：源数据集本身必须具有显著难度
```

## 5. Evaluation
**Marks**
1. 考虑位置偏差，需要将回答正反各一次输入LLMJudge进行评判，以检查Judge在Bench上位置偏差的表现。具体来说：两次评估中允许出现至多一次平局，不允许出现两种不同的偏好性评价。

**Objects**
1.  PromptJudge
	Vanilla：直接对LLM给出诸如“你现在是一位回答质量评估者，接下来，我会给你一系列输入，请决定哪个输入你认为更好”的提示。
	Arena-Hard：在以上的基础上加上CoT策略。“在回答下面的问题之前，请首先生成一个详细的回答，作为参考。然后，再评估以下两种候选答案的质量。”

2.  FTJudge

	PandaLM
	Prometheus 2
	JudgeLM
	Auto-J
	Skywork Critics

1. MultiAgent：
	ChatEval

**Metrics**
1. 在不同类别问题上评估LLMJudge的准确性。
2. 评估不同模型在JudgeBench上的表现。
3. 对弱模型通过回馈模型训练，得到一个专门的验证器来评判更强的模型是可能的。
4. 与其他数据集对比。
5. 探索“Judge”&“Solve”的难度差。
6. 探索偏差（见[[Judge Bench#^Bias|Bias]]）。

**Ques**
原文提到
```
使用 GPT-4o 生成所有响应对。这引入了对 GPT-4o 评委的偏见，因为生成的对对于 GPT-4o 自身来说本质上是具有挑战性的。
```
但直观上说，基础模型有能力生成正确的回答，必定由于基础模型对于该类问题有着较强的推理能力。与基础模型的强处相似的模型更可能取得优势。