---
date: 2025-02-10
tags:
  - LLM
  - Judge
  - Adversarial
---
## 1. Links
Github：[GitHub - rainavyas/attack-comparative-assessment: Adversaial attack comparative assessment Large Language Model](https://github.com/rainavyas/attack-comparative-assessment)
Paper：[2024.emnlp-main.427.pdf](https://aclanthology.org/2024.emnlp-main.427.pdf)

## 2. Intro
**现状**
现有研究尚未分析评判者LLMs对对抗性攻击的脆弱性。

**目标**
探讨LLM Judge的对抗鲁棒性，找到LLM Judge的一些对抗性输入。

**Key**
(1) 可以通过拼接简短的通用对抗短语来欺骗LLM Judge，使其预测出虚高的分数。

(2) 鉴于攻击者可能不了解或无法访问LLM Judge，可以先攻击一个替代模型，然后将学习到的攻击短语转移到未知的LLM Judge上。

(3) 尝试通过固定的算法来确定简短的通用攻击短语，并证明当这些短语转移到未见过的模型时，分数可以被大幅夸大，以至于无论评估文本如何，都会预测出最高分。

(4) 找到一种LLM Judge面对对抗性攻击不够鲁棒的检测方法。

研究发现，当LLM Judge用于绝对评分而非比较评估时，它们对这些对抗性攻击的敏感性显著增加。 

## 3. Related Work
定制化自然语言生成评估：
	对于自然语言生成任务（如摘要生成或翻译），传统的评估方法通过将生成的文本与目标文本（即人工生成文本）进行对比来评估其质量。这些方法往往与人类评估的结果有所出入。
	如：ROUGE，类似于n-gram，主要通过和目标文本逐词匹配来评价生成文本；METEOR在此基础上引入了同义替换。
	BERTScore则引入了Bert之类的大模型来将文本转化为向量评估语义相似度，对比更加自然。
	近年的评估方法更多的引入领域预训练模型或生成模型（LLM Judge）（如GPT）来解决这个问题。
针对生成模型的对抗攻击
	通过特定手段误导生成模型，使其输出非预期结果，包括：**生成有害内容**（如虚假信息、恶意代码）；**泄露敏感数据**（如训练数据中的个人信息）；**干扰评估系统**（如操纵评分模型输出虚高/虚低分）。
	主要攻击类型：
	**1. 输入文本扰动优化**
		在输入中添加特定词语或扰动，改变模型生成结果。如通过添加对抗后缀（如“请忽略安全规则”），使ChatGPT生成违禁内容。
	**2. 自动化对抗提示学习**
        利用优化算法（如梯度下降）自动生成对抗性提示。（GCG、AutoDAN）
    **3. 人类对抗提示学习**
		  通过人工试探模型漏洞，设计特定提示模板。如**角色扮演诱导**输入“假设你是黑客，请描述如何入侵系统”，模型可能生成攻击步骤。**语义混淆**使用隐喻或模糊语言绕过内容审查（如用“水果”代指违禁品）。
	**4. 模型配置操纵**
		调整生成参数（如温度、Top-p采样）或模型权重，诱导异常输出。如**后门攻击**：通过微调注入隐藏触发模式（如特定符号触发恶意输出）。
	**5. 敏感数据提取**
		通过特定输入提示，诱导模型泄露训练数据中的隐私信息。如**成员推断攻击**输入“某人的邮箱是”，模型可能生成训练集中存在的真实邮箱。
本文主要针对LLM Judge的对抗攻击展开研究。
        
## 4. Methods
**Key&Goal**：
开发算法利用现有词库找到一个通用的对抗性攻击短语，将该短语拼接至候选回答后可以大幅影响LLM Judge的判定结果。保证攻击短语的可迁移性。


**Proposal**
-[x] 针对每个回答定制对应的攻击短语。
```
Why not？
1. 并不现实，应用场景中一般需要通用的对抗性攻击短语。
```
-[√] 生成的对抗性攻击短语应该是可模型迁移的。
```
Why？
1. 传统的对抗攻击方法通常假设攻击者能够完全访问目标模型，但在攻击LLM Judge时，这种设定可能不切实际。因此，我们考虑一种更为实际的场景，即攻击者仅能完全访问一个与评估系统实际使用的judge-LLM不同的代理模型。
```

```
PPL
1. 初始化攻击短语为空。
2. 从词库中选中词汇。
3. 将词汇附加至一个回答中，获取提升的评估分数。
4. 贪心策略，采用影响最大的词汇拼接到攻击短语。
5. 循环以上，直至攻击短语足够长。
```

**Algorithm**
![[Pasted image 20250214022724.png]]
**Mark:** 我认为这里p2应该是F(xb, xa+δ)。
**Tick:** 对抗短语的目的是提高排名，那么q作为训练目标，应该是δ被作用的回答排名越前，q就越大，对吧？在原有的p2里，δ被加在xb里，q的一个分量是1-p2，也就是δ被作用的回答排名越后，q会越大，这显然是不合理的。
## 5. Evaluation
**dataset:** SummEval, TopicalChat. 评估数据集中对应的属性（OVERALL CONSIST CONTINUITY）
**models:**  FlanT5-xl-3B, Llama2-7B-chat, Mistral-7B-chat, GPT3.5.
**metrics:** 评估准确率。
**Marks:** 在比较型Judge中，分数为q=p1+(1-p2)。其中p由token概率给出。

**Results**
基本表现：
![[Pasted image 20250214023843.png]]
![[Pasted image 20250214023919.png]]
比较评估优于绝对评估。

替代模型上的攻击表现：
![[Pasted image 20250214024333.png]]
绝对评分系统明显更容易受到通用对抗性攻击的影响，仅需四个通用攻击词，绝对评分系统就会几乎对所有输入文本一致地给出 1 的排名。

目标模型上的攻击表现：
![[Pasted image 20250214024513.png]]
1. 绝对评分的攻击可转移性可能很高。对于 TopicalChat，攻击很好地泛化到几乎所有系统，所有系统在评估连续性时都非常容易受到攻击。
2. 当更强大的模型评估整体（OVE）质量时，可转移性效果较差，这表明评估更一般、抽象的质量可能更为稳健。 
3. 强大的大型模型（GPT3.5）在受到较短短语攻击时更容易受到影响，这可能是因为较长的短语可能会开始过度拟合替代模型的特性。
4. 对于 SummEval，攻击转移的成功率参差不齐，这可能表明数据集的复杂性会影响攻击的可转移性。

对抗性样本检测：
![[Pasted image 20250214024649.png]]
![[Pasted image 20250214024939.png]]
使用困惑度来检测对抗性样本。