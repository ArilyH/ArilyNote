#LLM #Start #DataWhale #NLP #BPE #Unigram #Tokenizer
# BPE-字对编码
BPE 的核心假设是：​**通过合并高频字符对，可以逐步构建出有意义的子词单元**。这种方法能够有效平衡词汇量和未登录词（OOV）问题。
1. ​**初始化词表**：
    - 初始词表包含所有字符（例如英文字母 `a-z` 和其他语言字符）。
    - 统计语料中所有字符对的频率。
2. ​**迭代合并**：
    - 在每轮迭代中，找到语料中出现频率最高的字符对（例如 `"h"` 和 `"a"` 合并为 `"ha"`）。
    - 将合并后的字符对加入词表，并更新语料中的字符对频率。
    - 重复上述过程，直到达到目标词表大小。
 不同语言的字符集差异很大（例如中文汉字、英文字母、日文假名等），直接对字符进行分词会导致词表膨胀。即**数据稀疏**问题。字节编码（UTF-8）将任何 Unicode 字符表示为 1 到 4 个字节，统一了所有语言的表示形式。在字节级别运行 BPE，可以避免对低频字符的依赖，从而提高模型的泛化能力。

# Unigram
在BPE过程结束后，我们得到一个分词表。
与仅仅根据频率进行拆分不同，一个更符合**机器学习直觉**的方法是在一个数据集上定义一个目标函数来捕捉一个好的分词的特征，这种基于目标函数的分词模型可以适应更好分词场景。
我们从分词表开始：