#LLM #Start #DataWhale #NLP #BPE #Unigram #Tokenizer
# BPE-字对编码
BPE 的核心假设是：​**通过合并高频字符对，可以逐步构建出有意义的子词单元**。这种方法能够有效平衡词汇量和未登录词（OOV）问题。
1. ​**初始化词表**：
    - 初始词表包含所有字符（例如英文字母 `a-z` 和其他语言字符）。
    - 统计语料中所有字符对的频率。
2. ​**迭代合并**：
    - 在每轮迭代中，找到语料中出现频率最高的字符对（例如 `"h"` 和 `"a"` 合并为 `"ha"`）。
    - 将合并后的字符对加入词表，并更新语料中的字符对频率。
    - 重复上述过程，直到达到目标词表大小。
 不同语言的字符集差异很大（例如中文汉字、英文字母、日文假名等），直接对字符进行分词会导致词表膨胀。即**数据稀疏**问题。字节编码（UTF-8）将任何 Unicode 字符表示为 1 到 4 个字节，统一了所有语言的表示形式。在字节级别运行 BPE，可以避免对低频字符的依赖，从而提高模型的泛化能力。

# Unigram
在BPE过程结束后，我们得到一个分词表。
与仅仅根据频率进行拆分不同，一个更符合**机器学习直觉**的方法是在一个数据集上定义一个目标函数来捕捉一个好的分词的特征，这种基于目标函数的分词模型可以适应更好分词场景。
我们从分词表开始，在语料库上训练这样一个模型：
1. **​Expectation**：
    - 对语料库中的每个词，计算所有可能的分割方式及其概率。
    - 选择概率最大的分词法作为本轮的分词结果。
2. **​Maximizaion**：
	- 根据Expectation中的分词结果重新计算子词的频率。
	- 
然后，计算这样一个损失函数：
	给定子词 $s$，对语料库 $V$ 中的所有词，计算其最优分割的对数似然之和：
	
	$L=sum()$
	
	其中，P(w) 是词 w 的最优分割的概率。
